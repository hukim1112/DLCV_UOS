{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1) Linear Regression from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Synthetic data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of data to be created\n",
    "m = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 독립변수입력) x1, x2 => 종속변수(정답) y \n",
    "x1 = np.random.randn(m, 1)\n",
    "x2 = np.random.randn(m, 1)\n",
    "y = x1 + x2 + np.random.rand(m, 1)\n",
    "\n",
    "plt.scatter(x1,x2,c=y, cmap='gray')\n",
    "plt.title(\"The distribution of created dataset \")\n",
    "plt.xlabel(\"input x1\")\n",
    "plt.ylabel(\"input x2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display \n",
    "display.Image(\"https://3gp10c1vpy442j63me73gy3s-wpengine.netdna-ssl.com/wp-content/uploads/2020/05/advertisement_3d-1-770x390.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프로그래밍, 디버깅 과정에서 항상 데이터의 Shape을 보는 습관을 들입시다.\n",
    "# 데이터의 수치는 보는 것은 코스트가 높은 행위이고, 생각보다 적은 정보를 준다.\n",
    "x = np.concatenate([x1,x2], axis=-1)\n",
    "print (x.shape)\n",
    "print (y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization of weights and bias\n",
    "w = np.array([1, 0])\n",
    "b = 0\n",
    "\n",
    "print(\"before\", w.shape)\n",
    "\n",
    "#항상 shape을 맞춰주는 것이 안전합니다.\n",
    "w = w[np.newaxis, :]\n",
    "print(\"after\", w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model : y = w*x + b\n",
    "def hypothesis(x, w, b):\n",
    "    pred = np.multiply(w, x) + b\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis(x,w,b)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction의 shape이 (100,2) 로 나옵니다. 정상적이라면 y의 shape은 (100, 1)이 되야 합니다.\n",
    "hypothesis(x,w,b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model : y = w*x + b\n",
    "def hypothesis(x, w, b):\n",
    "    pred = np.matmul(x, w.T) + b # matmul [100,2] * [2.1] => [100,1]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis(x,w,b)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis(x,w,b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비용함수를 정의해봅시다! 학습에는 사용되지 않지만 학습과정에서 loss가 줄어드는지 확인해볼 수 있겠군요.\n",
    "\n",
    "loss = (1/2)*(hypothesis(x, w, b) - y)**2 # 1/2 * (h(x) - y)^2\n",
    "print(\"loss's shape? : \", loss.shape) #중간중간 정상적인 shape인지를 확인해줍시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 데이터에 대한 loss를 위해 총합을 구해봅시다.\n",
    "total_loss = (1/m)*np.sum(loss)\n",
    "print(\"loss's shape? : \", total_loss.shape)\n",
    "print(\"value of loss : \", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(x, w, b, y):\n",
    "    loss = 1/2*(hypothesis(x, w, b) - y)**2 # loss = 1/2(h(x) - y)^2\n",
    "    total_loss = (1/m)*np.sum(loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost(x, w, b, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 학습에서는 gradient가 사용되죠?\n",
    "# 각 파라미터에 대한 gradient를 구해봅시다.\n",
    "# 자료에 gradient에 대한 수식이 있습니다.\n",
    "# d(cost)/dw =  (h(x) - y)(h(x))' = (h(x) - y)*x\n",
    "# d(cost)/db = (h(x) - y)(h(x))' = (h(x) -y)\n",
    "\n",
    "#shape 변화를 잘 트래킹하는 것이 요령입니다.\n",
    "print((hypothesis(x, w, b) - y).shape) # shape of (h(x) -y)\n",
    "print(x.shape) # shape of x\n",
    "print(((hypothesis(x, w, b) - y)*x).shape) # (h(x)-y)*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative가 2개만 나오는군요?\n",
    "def derivative(x, w, b, y):\n",
    "    dw = np.sum(((hypothesis(x, w, b) - y)*x))\n",
    "    db = np.sum(((hypothesis(x, w, b) - y)))\n",
    "    return dw, db\n",
    "\n",
    "derivative(x,w,b,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(x, w, b, y):\n",
    "    dw = np.sum(((hypothesis(x, w, b) - y)*x), axis=0)\n",
    "    db = np.sum(((hypothesis(x, w, b) - y)))\n",
    "    return dw, db\n",
    "\n",
    "derivative(x,w,b,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(x, w, b, y, alpha):\n",
    "    dw, db = derivative(x, w, b, y)\n",
    "    \n",
    "    w = w - alpha*dw # w := w + alpha * dw\n",
    "    b = b - alpha*db # b := b + alpha * db\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization of weights and bias\n",
    "w = np.array([0, 0])\n",
    "w = w[np.newaxis, :]\n",
    "b = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10번만 돌려봅시다.\n",
    "\n",
    "for i in range(10):\n",
    "    w, b = update(x,w,b,y, 0.0001)\n",
    "    loss = cost(x, w, b, y)\n",
    "    print(\"w b : \", w, b)\n",
    "    print(\"loss : \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100번 더 학습해봅시다.\n",
    "losses = []\n",
    "for i in range(500):\n",
    "    w, b = update(x,w,b,y, 0.0001)\n",
    "    loss = cost(x, w, b, y)\n",
    "    if (i+1)%100:\n",
    "        print(\"w b : \", w, b)\n",
    "        print(\"loss : \", loss)\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
